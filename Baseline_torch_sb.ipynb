{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20ZrIGlJz3o2"
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 8176,
     "status": "ok",
     "timestamp": 1710024775364,
     "user": {
      "displayName": "고건영",
      "userId": "13176787411488158617"
     },
     "user_tz": -540
    },
    "id": "b857OigSz5Az"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ignite.metrics import mIoU\n",
    "from torchmetrics.classification import ConfusionMatrix\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "# import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import namedtuple\n",
    "from sklearn.metrics import accuracy_score\n",
    "# import rasterio\n",
    "from copy import deepcopy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1710024775364,
     "user": {
      "displayName": "고건영",
      "userId": "13176787411488158617"
     },
     "user_tz": -540
    },
    "id": "goZno44sz7i2"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWvyaaxHChPf"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710026174587,
     "user": {
      "displayName": "고건영",
      "userId": "13176787411488158617"
     },
     "user_tz": -540
    },
    "id": "8A5fML8O-iFW"
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHJNmYHlCUEy"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEGMBg0_QrIY"
   },
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1710026175088,
     "user": {
      "displayName": "고건영",
      "userId": "13176787411488158617"
     },
     "user_tz": -540
    },
    "id": "jXoZmn6iMdVj"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from dataset import CustomDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import multiprocessing \n",
    "#multiprocessing.set_start_method('spawn')\n",
    "BASE = '.'\n",
    "train_meta = pd.read_csv(f'{BASE}/train_meta.csv')#.sample(n=30000,random_state=RANDOM_STATE)\n",
    "test_meta = pd.read_csv(f'{BASE}/test_meta.csv')\n",
    "\n",
    "# 데이터 위치\n",
    "IMAGES_PATH = f'{BASE}/train_img/'\n",
    "MASKS_PATH = f'{BASE}/train_mask/'\n",
    "\n",
    "# 가중치 저장 위치\n",
    "SAVE_PATH = f'{BASE}/train_output/'\n",
    "MODEL_SAVE = f'{SAVE_PATH}/best_UNet_Base_model.pth'\n",
    "\n",
    "# train : val = 8 : 2 나누기\n",
    "x_tr, x_val = train_test_split(train_meta, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# train : val\n",
    "images_train = [os.path.join(IMAGES_PATH, image) for image in x_tr['train_img'] ]\n",
    "masks_train = [os.path.join(MASKS_PATH, mask) for mask in x_tr['train_mask'] ]\n",
    "\n",
    "images_valid = [os.path.join(IMAGES_PATH, image) for image in x_val['train_img'] ]\n",
    "masks_valid = [os.path.join(MASKS_PATH, mask) for mask in x_val['train_mask'] ]\n",
    "\n",
    "WORKERS = 8\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = CustomDataset(images_train, masks_train, mode='train')\n",
    "valid_dataset = CustomDataset(images_valid, masks_valid, mode='valid')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "   dataset=train_dataset,\n",
    "   batch_size=BATCH_SIZE,\n",
    "   shuffle=True,\n",
    "   num_workers=WORKERS,\n",
    "   pin_memory=True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "   dataset=valid_dataset,\n",
    "   batch_size=BATCH_SIZE,\n",
    "   shuffle=False,\n",
    "   num_workers=WORKERS,\n",
    "   pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transforms = transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#         ])\n",
    "# val_transforms = transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1710026175088,
     "user": {
      "displayName": "고건영",
      "userId": "13176787411488158617"
     },
     "user_tz": -540
    },
    "id": "95awiQ48Cbqh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#WORKERS = 0 #\n",
    "EPOCHS = 100 # 훈련 epoch 지정\n",
    "BATCH_SIZE = 32 # batch size 지정\n",
    "IMAGE_SIZE = (256, 256) # 이미지 크기 지정\n",
    "\n",
    "# MAX_NORM = 3\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(DEVICE)\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "\n",
    "seed_everything(RANDOM_STATE) # SEED 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71BAohVtPmnP"
   },
   "source": [
    "### Model train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSJbSsDQy-vh"
   },
   "source": [
    "#### model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        inputs = F.sigmoid(inputs) # sigmoid를 통과한 출력이면 주석처리\n",
    "        \n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth) / (inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1710026176892,
     "user": {
      "displayName": "고건영",
      "userId": "13176787411488158617"
     },
     "user_tz": -540
    },
    "id": "t4sThsQmNgFF"
   },
   "outputs": [],
   "source": [
    "# torch init cache\n",
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    한 클래스에 대한 IoU를 계산합니다.\n",
    "    \"\"\"\n",
    "    intersection = np.logical_and(y_true, y_pred)\n",
    "    union = np.logical_or(y_true, y_pred)\n",
    "    iou_score = np.sum(intersection) / np.sum(union)\n",
    "    return iou_score\n",
    "\n",
    "def calculate_miou(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    산불이 있는 경우와 없는 경우, 두 클래스에 대한 평균 IoU(mIoU)를 계산합니다.\n",
    "    \"\"\"\n",
    "    y_pred = F.sigmoid(y_pred)\n",
    "    y_pred = np.where(y_pred > 0.2, 1, 0) # 임계값 처리\n",
    "    iou_fire = calculate_iou(y_true == 1, y_pred == 1)\n",
    "    \n",
    "    # 두 클래스에 대한 IoU 평균 계산\n",
    "    return iou_fire\n",
    "def pixel_accuracy (y_true, y_pred):\n",
    "    low = np.quantile(y_pred, 0.99)\n",
    "    y_pred = np.where(y_pred >  low, 1, 0) # 임계값 처리\n",
    "    sum_n = np.sum(np.logical_and(y_pred, y_true))\n",
    "    sum_t = np.sum(y_true)\n",
    "\n",
    "    if (sum_t == 0):\n",
    "        pixel_accuracy = 0\n",
    "    else:\n",
    "        pixel_accuracy = sum_n / sum_t\n",
    "    return pixel_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "model = smp.Unet('timm-efficientnet-b3', encoder_weights='imagenet',  classes=1, activation=None,in_channels=1)#, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16])\n",
    "#model.load_state_dict(torch.load(MODEL_SAVE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation\n",
    "# from datasets import load_dataset\n",
    "# # MaskFormer 모델 구성 수정\n",
    "# config = MaskFormerConfig.from_pretrained(\"facebook/maskformer-swin-base-ade\")\n",
    "# config.num_labels  = 1 \n",
    "# config.mask_feature_size = 1\n",
    "# model = MaskFormerForInstanceSegmentation(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model = smp.DeepLabV3Plus('resnet34', encoder_weights='imagenet',classes=1, activation='sigmoid',in_channels = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = smp.Unet(encoder_name='efficientnet-b0',encoder_weights ='imagenet',in_channels=10, classes=1,activation='sigmoid',)\n",
    "# weight_decay = 1e-4\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)#(params=model.parameters(), lr=0.0001)\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(DEVICE)\n",
    "# loss_fn = DiceLoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = torchvision.ops.sigmoid_focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495,
     "referenced_widgets": [
      "6bbb135fe51a416f832dc1f03639d1fb",
      "c91e9d469c044b61991b9f28c8f9d09e",
      "28088d2ffba24dc69e732a41aba9bc6b",
      "ed0c353831a64965a0f6fd28ede9e171",
      "c2d949edfd134daa9f0ecbabc23162a0",
      "399523c948d34ef6a68ea9113020c06c",
      "bffd9d14da8c4ecaab5e14e91189d9db",
      "c6c8ecda021c4b168d3063c750e1ad05",
      "20c3c2f34b474d968ee3ff8cdde41f59",
      "913249dc19844b9392a7d5b022e80d6a",
      "523a202649ec4042bc855d2ad8d5636c"
     ]
    },
    "executionInfo": {
     "elapsed": 46050,
     "status": "error",
     "timestamp": 1710026223376,
     "user": {
      "displayName": "고건영",
      "userId": "13176787411488158617"
     },
     "user_tz": -540
    },
    "id": "EsBjrh_mRjdt",
    "outputId": "72684dc2-e149-4383-b87d-7cca047deb66",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_val_loss = 0\n",
    "best_model = None\n",
    "early_stop = 0\n",
    "\n",
    "model.to(DEVICE)\n",
    "print('Train START')\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    print('Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "    print('-'*20)\n",
    "    train_loss, train_accs, train_ious = 0, [], []\n",
    "\n",
    "    #### train ####\n",
    "    for imgs, masks in tqdm(train_dataloader):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        masks = masks.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()  # 초기화\n",
    "        output = model(imgs)  # 예측\n",
    "        loss = loss_fn(output, masks)  # 순전파\n",
    "\n",
    "        loss.backward()  # 역전파\n",
    "        optimizer.step()  # 학습\n",
    "        \n",
    "        iou = calculate_miou(masks.cpu().detach().numpy(), output.cpu())\n",
    "        output = output.cpu().detach().numpy()\n",
    "        masks = masks.cpu().detach().numpy()\n",
    "        accuracy = pixel_accuracy(masks, output)  # mIou\n",
    "        train_accs.append(accuracy)\n",
    "        train_ious.append(iou)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    #### valid ####\n",
    "    model.eval()\n",
    "    val_loss, val_accs, val_ious = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in tqdm(val_dataloader):\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "\n",
    "            output = model(imgs)  # 예측\n",
    "\n",
    "            loss = loss_fn(output, masks)  # 순전파\n",
    "            iou = calculate_miou(masks.cpu().detach().numpy(), output.cpu())\n",
    "            output = output.cpu().detach().numpy()\n",
    "            masks = masks.cpu().detach().numpy()\n",
    "            accuracy = pixel_accuracy(masks, output)  # mIou\n",
    "            val_accs.append(accuracy)\n",
    "            val_ious.append(iou)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    train_acc = np.mean(train_accs)\n",
    "    train_iou = np.mean(train_ious)\n",
    "    val_acc = np.mean(val_accs)\n",
    "    val_iou = np.mean(val_ious)\n",
    "    val_loss /= len(val_dataloader)  # 평균 계산\n",
    "\n",
    "    print(f\"EPOCH: {epoch}, TRAIN LOSS: {train_loss:.6f}, TRAIN mIou: {train_iou:.6f}, TRAIN ACC: {train_acc:.6f}, VAL LOSS: {val_loss:.6f}, VAL mIou: {val_iou:.6f}, VAL ACC: {val_acc:.6f}\")\n",
    "\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step(val_loss)\n",
    "\n",
    "    if best_val_loss < val_iou:\n",
    "        print(\"Model Save\")\n",
    "        best_val_loss = val_iou\n",
    "        torch.save(model.state_dict(), MODEL_SAVE)\n",
    "        early_stop = 0\n",
    "    else:\n",
    "        early_stop += 1\n",
    "\n",
    "    # early stop\n",
    "    if early_stop > 10:\n",
    "        print(\"Early Stop\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM = 4\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "with torch.no_grad():\n",
    "    masksx = masks[NUM].cpu().detach().numpy()\n",
    "    masksx = np.reshape(masksx,(256,256,1))\n",
    "    # 이미지 표시\n",
    "    plt.imshow(masksx, cmap='gray')  # 흑백 이미지로 표시\n",
    "    plt.axis('off')  # 축 제거\n",
    "    plt.show()\n",
    "\n",
    "    image = imgs[NUM].cpu().detach().numpy()\n",
    "    image = np.reshape(image,(256,256,1))\n",
    "    # 이미지 표시\n",
    "    plt.imshow(image)  # 흑백 이미지로 표시\n",
    "    plt.axis('off')  # 축 제거\n",
    "    plt.show()\n",
    "    output = model(imgs)\n",
    "    masksx2 = output[NUM].cpu()#.detach().numpy()\n",
    "\n",
    "    masksx2=F.sigmoid(masksx2)\n",
    "    masksx2 = np.where(masksx2 > 0.2, 1, 0) # 임계값 처리\n",
    "    masksx2 = np.reshape(masksx2,(256,256,1))\n",
    "    # 이미지 표시\n",
    "    plt.imshow(masksx2, cmap='gray')  # 흑백 이미지로 표시\n",
    "    plt.axis('off')  # 축 제거\n",
    "    plt.show()\n",
    "    print(np.unique(masksx))\n",
    "    a = np.where(masksx == 1)\n",
    "    b = np.where(masksx2 == 1)\n",
    "    print(f'a:{a}')\n",
    "    print(f'b:{b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "model1 = smp.Unet('timm-efficientnet-b3', encoder_weights='imagenet',  classes=1, activation=None,in_channels=1)\n",
    "model2 = smp.Unet('timm-efficientnet-b3', encoder_weights='imagenet',  classes=1, activation=None,in_channels=1)\n",
    "model3 = smp.Unet('timm-efficientnet-b3', encoder_weights='imagenet',  classes=1, activation=None,in_channels=1)\n",
    "model4 = smp.Unet('timm-efficientnet-b3', encoder_weights='imagenet',  classes=1, activation=None,in_channels=1)\n",
    "model5 = smp.Unet('timm-efficientnet-b3', encoder_weights='imagenet',  classes=1, activation=None,in_channels=1)\n",
    "model6 = smp.Unet('timm-efficientnet-b3', encoder_weights='imagenet',  classes=1, activation=None,in_channels=1)\n",
    "model7 = smp.Unet('timm-efficientnet-b3', encoder_weights='imagenet',  classes=1, activation=None,in_channels=1)\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train START\n",
      "Epoch 1/100\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 292/840 [09:51<17:09,  1.88s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 모델 리스트 정의\n",
    "models = [model1.to(DEVICE), model2.to(DEVICE), model3.to(DEVICE), model4.to(DEVICE), model5.to(DEVICE), model6.to(DEVICE), model7.to(DEVICE)]  # 실제 모델 객체로 대체해야 합니다.\n",
    "\n",
    "# 모델 저장 경로 리스트 정의\n",
    "model_save_paths = [\n",
    "    f'{SAVE_PATH}/best_model1.pth',\n",
    "    f'{SAVE_PATH}/best_model2.pth',\n",
    "    f'{SAVE_PATH}/best_model3.pth',\n",
    "    f'{SAVE_PATH}/best_model4.pth',\n",
    "    f'{SAVE_PATH}/best_model5.pth',\n",
    "    f'{SAVE_PATH}/best_model6.pth',\n",
    "    f'{SAVE_PATH}/best_model7.pth'\n",
    "]\n",
    "optimizers = [torch.optim.AdamW(model.parameters(), lr=0.001) for model in models]\n",
    "lr_schedulers = [optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0.000001) for optimizer in optimizers]\n",
    "best_val_losses = [0] * len(models)\n",
    "early_stops = [0] * len(models)\n",
    "# 각 모델에 대한 학습 진행\n",
    "print('Train START')\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    for model in models:\n",
    "        model.train()\n",
    "    print('Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "    print('-'*20)\n",
    "    train_losses, train_accs, train_ious = [0] * len(models), [[] for _ in range(len(models))], [[] for _ in range(len(models))]\n",
    "    #### train ####\n",
    "    for images, masks_tr in tqdm(train_dataloader):\n",
    "        for idx, (model, optimizer) in enumerate(zip(models, optimizers)):\n",
    "            imgs =images[:,:,:,(idx)]\n",
    "            imgs =np.reshape(imgs, (BATCH_SIZE,1,256,256)).to(DEVICE)\n",
    "            masks = masks_tr.to(DEVICE)\n",
    "            optimizer.zero_grad()  # 초기화\n",
    "            output = model(imgs)  # 예측\n",
    "            loss = loss_fn(output, masks)  # 순전파\n",
    "            loss.backward()  # 역전파\n",
    "            optimizer.step()  # 학습\n",
    "            iou = calculate_miou(masks.cpu().detach().numpy(), output.cpu())\n",
    "            output = output.cpu().detach().numpy()\n",
    "            masks = masks.cpu().detach().numpy()\n",
    "            accuracy = pixel_accuracy(masks, output)  # mIou\n",
    "            train_accs[idx].append(accuracy)\n",
    "            train_ious[idx].append(iou)\n",
    "            train_losses[idx] += loss.item()\n",
    "\n",
    "    #### valid ####\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    val_losses, val_accs, val_ious = [0] * len(models), [[] for _ in range(len(models))], [[] for _ in range(len(models))]\n",
    "    with torch.no_grad():\n",
    "        for images, masks_tr in tqdm(val_dataloader):\n",
    "            for idx, model in enumerate(models):\n",
    "                imgs = images[:,:,:,(idx)]\n",
    "                imgs = np.reshape(imgs, (BATCH_SIZE,1,256,256)).to(DEVICE)\n",
    "                masks = masks_tr.to(DEVICE)\n",
    "                output = model(imgs)  # 예측\n",
    "                print(output.shape, masks.shape)\n",
    "                loss = loss_fn(output, masks)  # 순전파\n",
    "                iou = calculate_miou(masks.cpu().detach().numpy(), output.cpu())\n",
    "                output = output.cpu().detach().numpy()\n",
    "                masks = masks.cpu().detach().numpy()\n",
    "                accuracy = pixel_accuracy(masks, output)  # mIou\n",
    "                val_accs[idx].append(accuracy)\n",
    "                val_ious[idx].append(iou)\n",
    "                val_losses[idx] += loss.item()\n",
    "\n",
    "    for idx, (model, model_save_path, lr_scheduler) in enumerate(zip(models, model_save_paths, lr_schedulers)):\n",
    "        train_acc = np.mean(train_accs[idx])\n",
    "        train_iou = np.mean(train_ious[idx])\n",
    "        val_acc = np.mean(val_accs[idx])\n",
    "        val_iou = np.mean(val_ious[idx])\n",
    "        val_loss = val_losses[idx] / len(val_dataloader)  # 평균 계산\n",
    "        \n",
    "        print(f\"Model {idx+1} - EPOCH: {epoch}, TRAIN LOSS: {train_losses[idx]:.6f}, TRAIN mIou: {train_iou:.6f}, TRAIN ACC: {train_acc:.6f}, VAL LOSS: {val_loss:.6f}, VAL mIou: {val_iou:.6f}, VAL ACC: {val_acc:.6f}\")\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        if best_val_losses[idx] < val_iou:\n",
    "            print(f\"Model {idx+1} Save\")\n",
    "            best_val_losses[idx] = val_iou\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            early_stops[idx] = 0\n",
    "        else:\n",
    "            early_stops[idx] += 1\n",
    "        \n",
    "        # early stop\n",
    "        if early_stops[idx] > 10:\n",
    "            print(f\"Early Stop for Model {idx+1}\")\n",
    "            break\n",
    "        \n",
    "print(f\"Training finished for Model {idx+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerImageProcessor\n",
    "from datasets import load_dataset\n",
    "\n",
    "hf_dataset_identifier = \"segments/sidewalk-semantic\"\n",
    "ds = load_dataset(hf_dataset_identifier)\n",
    "processor = SegformerImageProcessor()\n",
    "\n",
    "id2label = {0: 'nofire', 1: 'fire'}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PIXEL_VALUE = 65535 # 이미지 정규화를 위한 픽셀 최대값\n",
    "def new_dataset(image):\n",
    "    \"\"\" make new image (threshold)\"\"\"\n",
    "    new_image = np.zeros_like(image[:, :, 0])\n",
    "    for (prob,idx) in [(0.5, 6), (0.4, 5), (0.4, 1), (0.1, 0), (0.1, 4)]:\n",
    "        new_image += prob * image[:, :, (idx)]\n",
    "    return new_image\n",
    "    \n",
    "def get_img_762bands(path):\n",
    "    image = tiff.imread(path)\n",
    "    img = np.float32(image) / MAX_PIXEL_VALUE  # 정규화\n",
    "    img = new_dataset(img)\n",
    "    img = np.repeat(img[:, :, np.newaxis], 3, axis=2)\n",
    "    #print(f\"Image shape: {img.shape}\") \n",
    "    return np.float32(img)\n",
    "\n",
    "def preprocess_mask(mask):\n",
    "    mask = torch.from_numpy(mask).float().unsqueeze(0)\n",
    "    mask = F.interpolate(mask, size=(64, 64), mode='bilinear', align_corners=False)\n",
    "    mask = mask.repeat(100, 1, 1, 1)  # 마스크를 100개로 복제\n",
    "    mask = (mask > 0.5).float()  # 이진 마스크로 변환\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MobileViTForSemanticSegmentation\n",
    "import tifffile as tiff\n",
    "# MaskFormer 모델 구성 수정\n",
    "\n",
    "model = MobileViTForSemanticSegmentation.from_pretrained(\"apple/deeplabv3-mobilevit-small\")\n",
    "\n",
    "num = 16266\n",
    "tes = f'{BASE}/train_img/train_img_{num}.tif'\n",
    "te = f'{BASE}/train_mask/train_mask_{num}.tif'\n",
    "test_image = get_img_762bands(tes)\n",
    "test_image = torch.from_numpy(test_image).float().unsqueeze(0)#.to(DEVICE)\n",
    "test_image = test_image.permute(0, 3, 1, 2)\n",
    "#print(test_image.shape)\n",
    "output= model(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, MobileViTModel, MobileViTConfig\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "config = MobileViTConfig(num_channels= 3, image_size = 256,)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"apple/deeplabv3-mobilevit-small\")\n",
    "model = MobileViTModel(config= config)\n",
    "model.return_dict = False\n",
    "#model.pixel_values=[1, 3, 512, 512]\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    print('output추론')\n",
    "\n",
    "# logits are of shape (batch_size, num_labels, height, width)\n",
    "logits = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs['pixel_values'].shape)\n",
    "plt.imshow(image)  # 흑백 이미지로 표시\n",
    "plt.axis('off')  # 축 제거\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs['pixel_values'].shape)\n",
    "plt.imshow(logits)  # 흑백 이미지로 표시\n",
    "plt.axis('off')  # 축 제거\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.logits.ahep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.logits.ahep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPAJgd2V0Fn0"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1710022640746,
     "user": {
      "displayName": "고건영",
      "userId": "13176787411488158617"
     },
     "user_tz": -540
    },
    "id": "sLS57pc-0H5a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_dict = {}\n",
    "\n",
    "# data load\n",
    "IMAGES_PATH = f'{BASE}/test_img/'\n",
    "images_test = [os.path.join(IMAGES_PATH, image) for image in test_meta['test_img'] ]\n",
    "masks_test = [os.path.join(MASKS_PATH, mask) for mask in test_meta['test_mask'] ]\n",
    "\n",
    "test_dataset = CustomDataset(images_test, masks_test, mode = 'test')\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "# load best model\n",
    "# smp\n",
    "#model = smp.FPN('timm-mobilenetv3_large_100', encoder_weights='imagenet', classes=1, activation=None,in_channels=1)\n",
    "model.load_state_dict(torch.load(MODEL_SAVE))\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, imgs in enumerate(tqdm(test_dataloader)):\n",
    "        name = test_meta['test_img'][idx]\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        output = model(imgs)\n",
    "        \n",
    "        output = output.cpu()#.detach().numpy()\n",
    "        output=F.sigmoid(output)\n",
    "        #low = np.quantile(output, 0.9999)\n",
    "        y_pred = np.where(output > 0.1, 1, 0) # 임계값 처리\n",
    "        y_pred = np.reshape(y_pred,(256,256))\n",
    "        \n",
    "        y_pred = y_pred.astype(np.uint8)\n",
    "\n",
    "        y_pred_dict[name] = y_pred\n",
    "#\n",
    "import joblib\n",
    "joblib.dump(y_pred_dict, f'{BASE}/y_pred-7.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOUQn6AUxE6YeNJV9MrP6qo",
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "20c3c2f34b474d968ee3ff8cdde41f59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "28088d2ffba24dc69e732a41aba9bc6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6c8ecda021c4b168d3063c750e1ad05",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_20c3c2f34b474d968ee3ff8cdde41f59",
      "value": 10
     }
    },
    "399523c948d34ef6a68ea9113020c06c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "523a202649ec4042bc855d2ad8d5636c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6bbb135fe51a416f832dc1f03639d1fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c91e9d469c044b61991b9f28c8f9d09e",
       "IPY_MODEL_28088d2ffba24dc69e732a41aba9bc6b",
       "IPY_MODEL_ed0c353831a64965a0f6fd28ede9e171"
      ],
      "layout": "IPY_MODEL_c2d949edfd134daa9f0ecbabc23162a0"
     }
    },
    "913249dc19844b9392a7d5b022e80d6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bffd9d14da8c4ecaab5e14e91189d9db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c2d949edfd134daa9f0ecbabc23162a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6c8ecda021c4b168d3063c750e1ad05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c91e9d469c044b61991b9f28c8f9d09e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_399523c948d34ef6a68ea9113020c06c",
      "placeholder": "​",
      "style": "IPY_MODEL_bffd9d14da8c4ecaab5e14e91189d9db",
      "value": " 20%"
     }
    },
    "ed0c353831a64965a0f6fd28ede9e171": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_913249dc19844b9392a7d5b022e80d6a",
      "placeholder": "​",
      "style": "IPY_MODEL_523a202649ec4042bc855d2ad8d5636c",
      "value": " 10/50 [00:45&lt;03:38,  5.47s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
